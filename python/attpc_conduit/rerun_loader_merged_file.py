import argparse
from pathlib import Path

import rerun as rr  # pip install rerun-sdk
from .core.pad_map import PadMap
from .core.config import Config
from .pipeline import init_detector_bounds, run_pipeline, init_detector_pad_plane
from .plot.histogram import Histogrammer
import h5py as h5
import numpy as np
import logging

RADIUS = 2.0

## Initialize histogrammer ##
grammer = Histogrammer()

grammer.add_2D("pid", 0.0, 5.0e3, 512, "dE/dx", 0.0, 3.0, 512, "Brho(Tm)")
grammer.add_2D("kinematics", 0.0, 180.0, 180, "Polar(deg)", 0.0, 3.0, 512, "Brho(Tm)")
grammer.add_1D("polar", 0.0, 180.0, 180, "polar(deg)")
## End histogrammer initialization ##

# handle text logs
logging.getLogger().addHandler(rr.LoggingHandler("logs/handler"))
logging.getLogger().setLevel(-1)
logging.info("This INFO log got added through the standard logging interface")

# The Rerun Viewer will always pass these two pieces of information:
# 1. The path to be loaded, as a positional arg.
# 2. A shared recording ID, via the `--recording-id` flag.
#
# It is up to you whether you make use of that shared recording ID or not.
# If you use it, the data will end up in the same recording as all other plugins interested in
# that file, otherwise you can just create a dedicated recording for it. Or both.
parser = argparse.ArgumentParser(
    description="""
This is an example executable data-loader plugin for the Rerun Viewer.
Any executable on your `$PATH` with a name that starts with `rerun-loader-` will be
treated as an external data-loader.

This particular one will read and analyze AT-TPC HDF5 files generated by the attpc_merger.
"""
)
parser.add_argument("filepath", type=str)
parser.add_argument("--recording-id", type=str)
args = parser.parse_args()


def get_event_range(trace_file: h5.File) -> tuple[int, int]:
    """
    The merger doesn't use attributes for legacy reasons, so everything is stored in datasets. Use this to retrieve the min and max event numbers.

    Parameters
    ----------
    trace_file: h5py.File
        File handle to a hdf5 file with AT-TPC traces

    Returns
    -------
    tuple[int, int]
        A pair of integers (first event number, last event number)
    """
    try:
        meta_group = trace_file["meta"]
        if not isinstance(meta_group, h5.Group):
            return (0, 0)
        meta_data = meta_group["meta"]
        if not isinstance(meta_data, h5.Dataset):
            return (0, 0)
        return (int(meta_data[0]), int(meta_data[2]))
    except Exception:
        return (0, 0)


def main() -> None:
    """The entry point for the rerun-loader-merged-file script"""
    print("here!")
    config = Config(Path("config.json"))
    pad_map = PadMap()

    path = Path(args.filepath)
    if not path.exists() or not path.is_file() or path.suffix != ".h5":
        print(f"here {path} {path.suffix}")
        exit(
            rr.EXTERNAL_DATA_LOADER_INCOMPATIBLE_EXIT_CODE
        )  # Indicates to Rerun that this file was not handled by this loader
    file = h5.File(path)
    get_group = None
    try:
        get_group = file["get"]
    except Exception:
        file.close()
        exit(rr.EXTERNAL_DATA_LOADER_INCOMPATIBLE_EXIT_CODE)
    if not isinstance(get_group, h5.Group):
        file.close()
        exit(rr.EXTERNAL_DATA_LOADER_INCOMPATIBLE_EXIT_CODE)
    min_event, max_event = get_event_range(file)
    if min_event == 0 and max_event == 0:
        file.close()
        exit(rr.EXTERNAL_DATA_LOADER_INCOMPATIBLE_EXIT_CODE)

    rr.init("attpc_h5_data", recording_id=args.recording_id)
    rr.stdout()  # Required for custom file loaders

    init_detector_bounds()
    # init_detector_pad_plane(pad_map) Currently doesn't work...

    for event_id in range(min_event, max_event):
        event_data = None
        try:
            event_data = get_group[f"evt{event_id}_data"]
        except Exception:
            continue

        if not isinstance(event_data, h5.Dataset):
            continue

        run_pipeline(event_id, event_data[:].copy(), grammer, pad_map, config)
